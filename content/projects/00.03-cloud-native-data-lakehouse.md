---
title: "00.03 cloud native data lakehouse"
date: 2025-10-05T21:16:53+05:30
draft: false
---

### cloud-native data lakehouse

#### what it is

a scalable, cloud-oriented lakehouse integrating batch and streaming ingestion, built on top of object storage and open table formats. this structure enables analytics and governance across petabyte-scale datasets.

#### what youâ€™ll build

- ingestion pipeline loading structured and semi-structured data (json, logs) into object storage
- transformation layer using spark or databricks with delta lake or apache iceberg for schema enforcement and version control
- partitioning and compaction strategy to optimize read performance
- secure access using cloud iam policies, encryption, and vpc isolation
- infrastructure deployment automated with terraform
- interactive querying using athena, bigquery, or synapse

#### approach

- create terraform modules to provision s3/gcs/adls and compute clusters
- configure spark for incremental upserts with merge operations
- establish role-based access control and encryption at rest/in-transit
- define lifecycle rules for cost control on infrequently accessed data
- integrate ci/cd for pipeline reproducibility and versioned deployments

#### tech stack

- cloud: aws, gcp, or azure
- storage: s3, gcs, or adls
- table format: delta lake or apache iceberg
- compute: spark on emr, dataproc, or databricks
- infrastructure: terraform

#### learning outcomes

- understand distributed cloud architectures for data platforms
- learn about lakehouse paradigms combining data lakes and warehouses
- master infrastructure as code and automated provisioning
- develop secure, scalable pipelines with cost-aware design
- implement reproducible, multi-environment deployments
